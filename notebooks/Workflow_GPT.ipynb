{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import os\n",
    "import random\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.core import Settings\n",
    "\n",
    "llm = OpenAI(model=\"gpt-4\")\n",
    "Settings.llm = llm\n",
    "Settings.embed_model = OpenAIEmbedding(model=\"text-embedding-3-small\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import SimpleDirectoryReader\n",
    "wiki_titles = [\"Toronto\", \"Seattle\", \"Chicago\", \"Boston\", \"Houston\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pathlib import Path\n",
    "\n",
    "# import requests\n",
    "\n",
    "# for title in wiki_titles:\n",
    "#     response = requests.get(\n",
    "#         \"https://en.wikipedia.org/w/api.php\",\n",
    "#         params={\n",
    "#             \"action\": \"query\",\n",
    "#             \"format\": \"json\",\n",
    "#             \"titles\": title,\n",
    "#             \"prop\": \"extracts\",\n",
    "#             # 'exintro': True,\n",
    "#             \"explaintext\": True,\n",
    "#         },\n",
    "#     ).json()\n",
    "#     page = next(iter(response[\"query\"][\"pages\"].values()))\n",
    "#     wiki_text = page[\"extract\"]\n",
    "\n",
    "#     data_path = Path(\"data/wiki\")\n",
    "#     if not data_path.exists():\n",
    "#         Path.mkdir(data_path)\n",
    "\n",
    "#     with open(data_path / f\"{title}.txt\", \"w\") as fp:\n",
    "#         fp.write(wiki_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "city_docs = {}\n",
    "for wiki_title in wiki_titles:\n",
    "    city_docs[wiki_title] = SimpleDirectoryReader(\n",
    "        input_files=[f\"data/wiki/{wiki_title}.txt\"]\n",
    "    ).load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import VectorStoreIndex\n",
    "from llama_index.agent.openai import OpenAIAgent\n",
    "from llama_index.core.tools import QueryEngineTool, ToolMetadata\n",
    "from llama_index.core import VectorStoreIndex\n",
    "\n",
    "# Build tool dictionary\n",
    "tool_dict = {}\n",
    "\n",
    "for wiki_title in wiki_titles:\n",
    "    # build vector index\n",
    "    vector_index = VectorStoreIndex.from_documents(\n",
    "        city_docs[wiki_title],\n",
    "    )\n",
    "    # define query engines\n",
    "    vector_query_engine = vector_index.as_query_engine(llm=llm)\n",
    "\n",
    "    # define tools\n",
    "    vector_tool = QueryEngineTool(\n",
    "        query_engine=vector_query_engine,\n",
    "        metadata=ToolMetadata(\n",
    "            name=wiki_title,\n",
    "            description=(\"Useful for questions related to\" f\" {wiki_title}\"),\n",
    "        ),\n",
    "    )\n",
    "    tool_dict[wiki_title] = vector_tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "dict_keys(['Toronto', 'Seattle', 'Chicago', 'Boston', 'Houston'])\n",
      "Useful for questions related to Toronto\n",
      "Toronto is located in the Canadian province of Ontario, at the western end of Lake Ontario.\n",
      "[<llama_index.core.tools.query_engine.QueryEngineTool object at 0x17e75f440>, <llama_index.core.tools.query_engine.QueryEngineTool object at 0x17f825d60>, <llama_index.core.tools.query_engine.QueryEngineTool object at 0x17f825b50>, <llama_index.core.tools.query_engine.QueryEngineTool object at 0x17f8257f0>, <llama_index.core.tools.query_engine.QueryEngineTool object at 0x17f06dc40>]\n"
     ]
    }
   ],
   "source": [
    "print(len(tool_dict))\n",
    "print(tool_dict.keys())\n",
    "print(tool_dict[\"Toronto\"].metadata.description)\n",
    "print(tool_dict[\"Toronto\"].query_engine.query(\"Where is Toronto?\"))\n",
    "print(list(tool_dict.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define an \"object\" index and retriever over these tools\n",
    "from llama_index.core import VectorStoreIndex\n",
    "from llama_index.core.objects import ObjectIndex\n",
    "\n",
    "tool_index = ObjectIndex.from_objects(\n",
    "    list(tool_dict.values()),\n",
    "    index_cls=VectorStoreIndex,\n",
    ")\n",
    "tool_retriever = tool_index.as_retriever(similarity_top_k=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.llms import ChatMessage\n",
    "from llama_index.core import ChatPromptTemplate\n",
    "from typing import List\n",
    "\n",
    "GEN_SYS_PROMPT_STR = \"\"\"\\\n",
    "Task information is given below. \n",
    "\n",
    "Given the task, please generate a system prompt for an OpenAI-powered bot to solve this task: \n",
    "{task} \\\n",
    "\"\"\"\n",
    "\n",
    "gen_sys_prompt_messages = [\n",
    "    ChatMessage(\n",
    "        role=\"system\",\n",
    "        content=\"You are helping to build a system prompt for another bot.\",\n",
    "    ),\n",
    "    ChatMessage(role=\"user\", content=GEN_SYS_PROMPT_STR),\n",
    "]\n",
    "\n",
    "GEN_SYS_PROMPT_TMPL = ChatPromptTemplate(gen_sys_prompt_messages)\n",
    "\n",
    "\n",
    "agent_cache = {}\n",
    "\n",
    "from llama_index.core.agent import ReActAgent\n",
    "\n",
    "def create_system_prompt(task: str):\n",
    "    \"\"\"Create system prompt for another agent given an input task.\"\"\"\n",
    "    llm = OpenAI(llm=\"gpt-4\")\n",
    "    fmt_messages = GEN_SYS_PROMPT_TMPL.format_messages(task=task)\n",
    "    response = llm.chat(fmt_messages)\n",
    "    return response.message.content\n",
    "\n",
    "\n",
    "def get_tools(task: str):\n",
    "    \"\"\"Get the set of relevant tools to use given an input task.\"\"\"\n",
    "    subset_tools = tool_retriever.retrieve(task)\n",
    "    return [t.metadata.name for t in subset_tools]\n",
    "\n",
    "\n",
    "def create_agent(system_prompt: str, tool_names: List[str]):\n",
    "    \"\"\"Create an agent given a system prompt and an input set of tools.\"\"\"\n",
    "    llm = OpenAI(model=\"gpt-4\")\n",
    "    try:\n",
    "        # get the list of tools\n",
    "        input_tools = [tool_dict[tn] for tn in tool_names]\n",
    "\n",
    "        agent = ReActAgent.from_tools(input_tools, llm=llm, verbose=True)\n",
    "        print(agent)\n",
    "        agent_cache[\"agent\"] = agent\n",
    "        return_msg = \"Agent created successfully.\"\n",
    "    except Exception as e:\n",
    "        return_msg = f\"An error occurred when building an agent. Here is the error: {repr(e)}\"\n",
    "    return return_msg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.tools import FunctionTool\n",
    "\n",
    "system_prompt_tool = FunctionTool.from_defaults(fn=create_system_prompt)\n",
    "get_tools_tool = FunctionTool.from_defaults(fn=get_tools)\n",
    "create_agent_tool = FunctionTool.from_defaults(fn=create_agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "GPT_BUILDER_SYS_STR = \"\"\"\\\n",
    "You are helping to construct an agent given a user-specified task. You should generally use the tools in this order to build the agent.\n",
    "\n",
    "1) Create system prompt tool: to create the system prompt for the agent.\n",
    "2) Get tools tool: to fetch the candidate set of tools to use.\n",
    "3) Create agent tool: to create the final agent.\n",
    "\"\"\"\n",
    "\n",
    "prefix_msgs = [ChatMessage(role=\"system\", content=GPT_BUILDER_SYS_STR)]\n",
    "\n",
    "\n",
    "builder_agent = OpenAIAgent.from_tools(\n",
    "    tools=[system_prompt_tool, get_tools_tool, create_agent_tool],\n",
    "    prefix_messages=prefix_msgs,\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added user message to memory: Build an agent that can tell me about Toronto.\n",
      "=== Calling Function ===\n",
      "Calling function: create_system_prompt with args: {\n",
      "  \"task\": \"tell me about Toronto\"\n",
      "}\n",
      "Got output: \"Generate a brief summary about Toronto, including its history, landmarks, culture, and notable features.\"\n",
      "========================\n",
      "\n",
      "=== Calling Function ===\n",
      "Calling function: get_tools with args: {\n",
      "  \"task\": \"tell me about Toronto\"\n",
      "}\n",
      "Got output: ['Toronto']\n",
      "========================\n",
      "\n",
      "=== Calling Function ===\n",
      "Calling function: create_agent with args: {\n",
      "  \"system_prompt\": \"Generate a brief summary about Toronto, including its history, landmarks, culture, and notable features.\",\n",
      "  \"tool_names\": [\"Toronto\"]\n",
      "}\n",
      "<llama_index.core.agent.react.base.ReActAgent object at 0x282a342c0>\n",
      "Got output: Agent created successfully.\n",
      "========================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Response(response='The agent has been successfully created. It will provide you with a brief summary about Toronto, including its history, landmarks, culture, and notable features.', source_nodes=[], metadata=None)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "builder_agent.query(\"Build an agent that can tell me about Toronto.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Running step 98329262-eeb6-4686-938f-2bc9493b5ba7. Step input: Where is Toronto?\n",
      "\u001b[1;3;38;5;200mThought: The user is asking about the location of Toronto. I can use the Toronto tool to get this information.\n",
      "Action: Toronto\n",
      "Action Input: {'input': 'Where is Toronto?'}\n",
      "\u001b[0m\u001b[1;3;34mObservation: Toronto is located in the Canadian province of Ontario, on the northwestern shore of Lake Ontario.\n",
      "\u001b[0m> Running step 3e9c69cc-399a-440b-8063-8732293b37de. Step input: None\n",
      "\u001b[1;3;38;5;200mThought: I can answer without using any more tools. I'll use the user's language to answer.\n",
      "Answer: Toronto is located in the Canadian province of Ontario, on the northwestern shore of Lake Ontario.\n",
      "\u001b[0mToronto is located in the Canadian province of Ontario, on the northwestern shore of Lake Ontario.\n"
     ]
    }
   ],
   "source": [
    "city_agent = agent_cache[\"agent\"]\n",
    "print(city_agent.query(\"Where is Toronto?\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "autorag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
