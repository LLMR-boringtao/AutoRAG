{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "from llama_index.llms.openai import OpenAI\n",
    "llm = OpenAI(model=\"gpt-4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from llama_index.core.workflow import Event\n",
    "from llama_index.core.schema import NodeWithScore\n",
    "\n",
    "\n",
    "class RetrieverEvent(Event):\n",
    "    \"\"\"Result of running retrieval\"\"\"\n",
    "\n",
    "    nodes: list[NodeWithScore]\n",
    "\n",
    "\n",
    "class CreateCitationsEvent(Event):\n",
    "    \"\"\"Add citations to the nodes.\"\"\"\n",
    "\n",
    "    nodes: list[NodeWithScore]\n",
    "\n",
    "from llama_index.core.workflow import (\n",
    "    Context,\n",
    "    Workflow,\n",
    "    StartEvent,\n",
    "    StopEvent,\n",
    "    step,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating new index...\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core import (\n",
    "    SimpleDirectoryReader,\n",
    "    VectorStoreIndex,\n",
    "    StorageContext,\n",
    "    load_index_from_storage\n",
    ")\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "\n",
    "def load_or_create_index(directory_path, persist_dir):\n",
    "        if os.path.exists(persist_dir):\n",
    "            print(\"Loading existing index...\")\n",
    "            storage_context = StorageContext.from_defaults(persist_dir=persist_dir)\n",
    "            index = load_index_from_storage(storage_context)\n",
    "        else:\n",
    "            print(\"Creating new index...\")\n",
    "            documents = SimpleDirectoryReader(directory_path, recursive=True).load_data()\n",
    "            index = VectorStoreIndex.from_documents(\n",
    "                documents=documents,\n",
    "                embed_model=OpenAIEmbedding(model_name=\"text-embedding-3-small\"),\n",
    "            )\n",
    "            index.storage_context.persist(persist_dir=persist_dir)\n",
    "        return index\n",
    "\n",
    "index = load_or_create_index(\n",
    "            \"data/get-started/\",\n",
    "            \"storage\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<llama_index.core.indices.vector_store.base.VectorStoreIndex object at 0x176ff31a0>\n"
     ]
    }
   ],
   "source": [
    "print(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.prompts import PromptTemplate\n",
    "\n",
    "CITATION_QA_TEMPLATE = PromptTemplate(\n",
    "    \"Please provide an answer based solely on the provided sources. \"\n",
    "    \"When referencing information from a source, \"\n",
    "    \"cite the appropriate source(s) using their corresponding numbers. \"\n",
    "    \"Every answer should include at least one source citation. \"\n",
    "    \"Only cite a source when you are explicitly referencing it. \"\n",
    "    \"If none of the sources are helpful, you should indicate that. \"\n",
    "    \"For example:\\n\"\n",
    "    \"Source 1:\\n\"\n",
    "    \"The sky is red in the evening and blue in the morning.\\n\"\n",
    "    \"Source 2:\\n\"\n",
    "    \"Water is wet when the sky is red.\\n\"\n",
    "    \"Query: When is water wet?\\n\"\n",
    "    \"Answer: Water will be wet when the sky is red [2], \"\n",
    "    \"which occurs in the evening [1].\\n\"\n",
    "    \"Now it's your turn. Below are several numbered sources of information:\"\n",
    "    \"\\n------\\n\"\n",
    "    \"{context_str}\"\n",
    "    \"\\n------\\n\"\n",
    "    \"Query: {query_str}\\n\"\n",
    "    \"Answer: \"\n",
    ")\n",
    "\n",
    "CITATION_REFINE_TEMPLATE = PromptTemplate(\n",
    "    \"Please provide an answer based solely on the provided sources. \"\n",
    "    \"When referencing information from a source, \"\n",
    "    \"cite the appropriate source(s) using their corresponding numbers. \"\n",
    "    \"Every answer should include at least one source citation. \"\n",
    "    \"Only cite a source when you are explicitly referencing it. \"\n",
    "    \"If none of the sources are helpful, you should indicate that. \"\n",
    "    \"For example:\\n\"\n",
    "    \"Source 1:\\n\"\n",
    "    \"The sky is red in the evening and blue in the morning.\\n\"\n",
    "    \"Source 2:\\n\"\n",
    "    \"Water is wet when the sky is red.\\n\"\n",
    "    \"Query: When is water wet?\\n\"\n",
    "    \"Answer: Water will be wet when the sky is red [2], \"\n",
    "    \"which occurs in the evening [1].\\n\"\n",
    "    \"Now it's your turn. \"\n",
    "    \"We have provided an existing answer: {existing_answer}\"\n",
    "    \"Below are several numbered sources of information. \"\n",
    "    \"Use them to refine the existing answer. \"\n",
    "    \"If the provided sources are not helpful, you will repeat the existing answer.\"\n",
    "    \"\\nBegin refining!\"\n",
    "    \"\\n------\\n\"\n",
    "    \"{context_msg}\"\n",
    "    \"\\n------\\n\"\n",
    "    \"Query: {query_str}\\n\"\n",
    "    \"Answer: \"\n",
    ")\n",
    "\n",
    "DEFAULT_CITATION_CHUNK_SIZE = 512\n",
    "DEFAULT_CITATION_CHUNK_OVERLAP = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.schema import (\n",
    "    MetadataMode,\n",
    "    NodeWithScore,\n",
    "    TextNode,\n",
    ")\n",
    "\n",
    "from llama_index.core.response_synthesizers import (\n",
    "    ResponseMode,\n",
    "    get_response_synthesizer,\n",
    ")\n",
    "\n",
    "from typing import Union, List\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "\n",
    "\n",
    "class CitationQueryEngineWorkflow(Workflow):\n",
    "    @step(pass_context=True)\n",
    "    async def retrieve(\n",
    "        self, ctx: Context, ev: StartEvent\n",
    "    ) -> RetrieverEvent:\n",
    "        \"Entry point for RAG, triggered by a StartEvent with `query`.\"\n",
    "        query = ev.get(\"query\")\n",
    "        if not query:\n",
    "            return None\n",
    "\n",
    "        print(f\"Query the database with: {query}\")\n",
    "\n",
    "        # store the query in the global context\n",
    "        ctx.data[\"query\"] = query\n",
    "\n",
    "        if ev.index is None:\n",
    "            print(\"Index is empty, load some documents before querying!\")\n",
    "            return None\n",
    "        else:\n",
    "            print(\"Index is not empty, proceed with querying!\")\n",
    "\n",
    "        retriever = ev.index.as_retriever(similarity_top_k=2)\n",
    "        nodes = retriever.retrieve(query)\n",
    "        print(f\"Retrieved {len(nodes)} nodes.\")\n",
    "        return RetrieverEvent(nodes=nodes)\n",
    "    \n",
    "    @step(pass_context=True)\n",
    "    async def create_citation_nodes(\n",
    "        self, ctx: Context, ev: RetrieverEvent\n",
    "    ) -> CreateCitationsEvent:\n",
    "        \"\"\"\n",
    "        Modify retrieved nodes to create granular sources for citations.\n",
    "\n",
    "        Takes a list of NodeWithScore objects and splits their content\n",
    "        into smaller chunks, creating new NodeWithScore objects for each chunk.\n",
    "        Each new node is labeled as a numbered source, allowing for more precise\n",
    "        citation in query results.\n",
    "\n",
    "        Args:\n",
    "            nodes (List[NodeWithScore]): A list of NodeWithScore objects to be processed.\n",
    "\n",
    "        Returns:\n",
    "            List[NodeWithScore]: A new list of NodeWithScore objects, where each object\n",
    "            represents a smaller chunk of the original nodes, labeled as a source.\n",
    "        \"\"\"\n",
    "        nodes = ev.nodes\n",
    "\n",
    "        new_nodes: List[NodeWithScore] = []\n",
    "\n",
    "        text_splitter = SentenceSplitter(\n",
    "            chunk_size=DEFAULT_CITATION_CHUNK_SIZE,\n",
    "            chunk_overlap=DEFAULT_CITATION_CHUNK_OVERLAP,\n",
    "        )\n",
    "\n",
    "        for node in nodes:\n",
    "            text_chunks = text_splitter.split_text(\n",
    "                node.node.get_content(metadata_mode=MetadataMode.NONE)\n",
    "            )\n",
    "\n",
    "            for text_chunk in text_chunks:\n",
    "                text = f\"Source {len(new_nodes)+1}:\\n{text_chunk}\\n\"\n",
    "\n",
    "                new_node = NodeWithScore(\n",
    "                    node=TextNode.parse_obj(node.node), score=node.score\n",
    "                )\n",
    "                new_node.node.text = text\n",
    "                new_nodes.append(new_node)\n",
    "        return CreateCitationsEvent(nodes=new_nodes)\n",
    "\n",
    "    @step(pass_context=True)\n",
    "    async def synthesize(\n",
    "        self, ctx: Context, ev: CreateCitationsEvent\n",
    "    ) -> StopEvent:\n",
    "        \"\"\"Return a streaming response using the retrieved nodes.\"\"\"\n",
    "        llm = OpenAI(model=\"gpt-4\")\n",
    "        query = ctx.data.get(\"query\")\n",
    "        print(f\"Synthesizing response for query: {query}\")\n",
    "\n",
    "        synthesizer = get_response_synthesizer(\n",
    "            llm=llm,\n",
    "            text_qa_template=CITATION_QA_TEMPLATE,\n",
    "            refine_template=CITATION_REFINE_TEMPLATE,\n",
    "            response_mode=ResponseMode.COMPACT,\n",
    "            use_async=True,\n",
    "        )\n",
    "\n",
    "        response = await synthesizer.asynthesize(query, nodes=ev.nodes)\n",
    "        print(f\"Response: {response}\")\n",
    "        return StopEvent(result=response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CitationQueryEngineWorkflow.html\n"
     ]
    }
   ],
   "source": [
    "from llama_index.utils.workflow import draw_all_possible_flows\n",
    "\n",
    "draw_all_possible_flows(CitationQueryEngineWorkflow,filename=\"CitationQueryEngineWorkflow.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = CitationQueryEngineWorkflow()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query the database with: What is streamlit?\n",
      "Index is not empty, proceed with querying!\n",
      "Retrieved 2 nodes.\n",
      "Synthesizing response for query: What is streamlit?\n",
      "Response: Streamlit is a tool for creating interactive apps using Python. It allows you to write apps in the same way you write plain Python scripts. You can add Streamlit commands into a normal Python script and run it with `streamlit run`. This will spin up a local Streamlit server and open your app in a new tab in your default web browser. The app can include charts, text, widgets, tables, and more. Streamlit's architecture has a unique data flow: any time something must be updated on the screen, Streamlit reruns your entire Python script from top to bottom [1][2].\n"
     ]
    }
   ],
   "source": [
    "result = await w.run(query=\"What is streamlit?\", index=index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Streamlit is a tool for creating interactive apps using Python. It allows you to write apps in the same way you write plain Python scripts. You can add Streamlit commands into a normal Python script and run it with `streamlit run`. This will spin up a local Streamlit server and open your app in a new tab in your default web browser. The app can include charts, text, widgets, tables, and more. Streamlit's architecture has a unique data flow: any time something must be updated on the screen, Streamlit reruns your entire Python script from top to bottom [1][2]."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Markdown, display\n",
    "\n",
    "display(Markdown(f\"{result}\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source 2:\n",
      "Data flow\n",
      "\n",
      "Streamlit's architecture allows you to write apps the same way you write plain\n",
      "Python scripts. To unlock this, Streamlit apps have a unique data flow: any\n",
      "time something must be updated on the screen, Streamlit reruns your entire\n",
      "Python script from top to bottom.\n",
      "\n",
      "This can happen in two situations:\n",
      "\n",
      "- Whenever you modify your app's source code.\n",
      "\n",
      "- Whenever a user interacts with widgets in the app. For example, when dragging\n",
      "  a slider, entering text in an input box, or clicking a button.\n",
      "\n",
      "Whenever a callback is passed to a widget via the `on_change` (or `on_click`) parameter, the callback will always run before the rest of your script. For details on the Callbacks API, please refer to our Session State API Reference Guide.\n",
      "\n",
      "And to make all of this fast and seamless, Streamlit does some heavy lifting\n",
      "for you behind the scenes. A big player in this story is the\n",
      "`@st.cache_data` decorator, which allows developers to skip certain\n",
      "costly computations when their apps rerun. We'll cover caching later in this\n",
      "page.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(result.source_nodes[1].node.get_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = index.as_retriever()\n",
    "retrieved_nodes = retriever.retrieve(\"What is llama_index?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NodeWithScore(node=TextNode(id_='6da6d750-6260-4433-83b5-dabf42afc7d4', embedding=None, metadata={'file_path': '/Users/boringtao/Projects/AutoRAG/notebooks/data/get-started/fundamentals/_index.md', 'file_name': '_index.md', 'file_size': 1254, 'creation_date': '2024-08-21', 'last_modified_date': '2024-08-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='8225bafb-71a4-4a8e-94bf-5e67c9b5eeb0', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'file_path': '/Users/boringtao/Projects/AutoRAG/notebooks/data/get-started/fundamentals/_index.md', 'file_name': '_index.md', 'file_size': 1254, 'creation_date': '2024-08-21', 'last_modified_date': '2024-08-21'}, hash='869fdb122bfd92504d4cf9adb5958b115175df93719bd96aaf218feb09986665')}, text='---\\ntitle: Fundamental concepts\\nslug: /get-started/fundamentals\\n---', mimetype='text/plain', start_char_idx=0, end_char_idx=67, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), score=0.22878376177300447), NodeWithScore(node=TextNode(id_='57840454-dc0c-44b1-a616-0b5898f84e44', embedding=None, metadata={'file_path': '/Users/boringtao/Projects/AutoRAG/notebooks/data/get-started/fundamentals/main-concepts.md', 'file_name': 'main-concepts.md', 'file_size': 14293, 'creation_date': '2024-08-21', 'last_modified_date': '2024-08-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='49cf940c-e7f9-434c-b3f0-279449a00f58', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'file_path': '/Users/boringtao/Projects/AutoRAG/notebooks/data/get-started/fundamentals/main-concepts.md', 'file_name': 'main-concepts.md', 'file_size': 14293, 'creation_date': '2024-08-21', 'last_modified_date': '2024-08-21'}, hash='dd2d42c40c09583120fd553b9e168eb5d350b224515abc50b807317fccfd229b')}, text='---\\ntitle: Basic concepts of Streamlit\\nslug: /get-started/fundamentals/main-concepts\\n---', mimetype='text/plain', start_char_idx=0, end_char_idx=88, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), score=0.20694834724132213)]\n"
     ]
    }
   ],
   "source": [
    "print(retrieved_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.data_structs import Node\n",
    "from llama_index.core.response_synthesizers import ResponseMode\n",
    "from llama_index.core import get_response_synthesizer\n",
    "\n",
    "response_synthesizer = get_response_synthesizer(\n",
    "    response_mode=ResponseMode.COMPACT\n",
    ")\n",
    "\n",
    "query_engine = index.as_query_engine(response_synthesizer=response_synthesizer)\n",
    "response = query_engine.query(\"What is streamlit\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Streamlit is a tool that allows users to easily create interactive web applications using Python scripts. By adding Streamlit commands to a Python script and running it with `streamlit run`, a local Streamlit server is started, and the app can be viewed in a web browser. Streamlit provides various commands like `st.text` for adding text and `st.line_chart` for drawing line charts, enabling users to create a wide range of visualizations and interactive elements in their apps.\n"
     ]
    }
   ],
   "source": [
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "autorag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
