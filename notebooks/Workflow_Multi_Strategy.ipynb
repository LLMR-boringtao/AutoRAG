{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from llama_index.core import (\n",
    "    SimpleDirectoryReader,\n",
    "    VectorStoreIndex,\n",
    "    StorageContext,\n",
    "    load_index_from_storage\n",
    ")\n",
    "from llama_index.core.agent.react import ReActAgent\n",
    "from llama_index.core.workflow import (\n",
    "    step,\n",
    "    Context,\n",
    "    Workflow,\n",
    "    Event,\n",
    "    StartEvent,\n",
    "    StopEvent\n",
    ")\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.core.postprocessor.rankGPT_rerank import RankGPTRerank\n",
    "from llama_index.core.query_engine import RetrieverQueryEngine\n",
    "from llama_index.core.chat_engine import SimpleChatEngine\n",
    "from llama_index.utils.workflow import draw_all_possible_flows\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class JudgeEvent(Event):\n",
    "    query: str\n",
    "\n",
    "class BadQueryEvent(Event):\n",
    "    query: str\n",
    "\n",
    "class NaiveRAGEvent(Event):\n",
    "    query: str\n",
    "\n",
    "class HighTopKEvent(Event):\n",
    "    query: str\n",
    "\n",
    "class RerankEvent(Event):\n",
    "    query: str\n",
    "\n",
    "class ResponseEvent(Event):\n",
    "    query: str\n",
    "    response: str\n",
    "\n",
    "class SummarizeEvent(Event):\n",
    "    query: str\n",
    "    response: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ComplicatedWorkflow(Workflow):\n",
    "\n",
    "    def load_or_create_index(self, directory_path, persist_dir):\n",
    "        if os.path.exists(persist_dir):\n",
    "            print(\"Loading existing index...\")\n",
    "            storage_context = StorageContext.from_defaults(persist_dir=persist_dir)\n",
    "            index = load_index_from_storage(storage_context)\n",
    "        else:\n",
    "            print(\"Creating new index...\")\n",
    "            documents = SimpleDirectoryReader(directory_path, recursive=True).load_data()\n",
    "            index = VectorStoreIndex.from_documents(documents)\n",
    "            index.storage_context.persist(persist_dir=persist_dir)\n",
    "        return index\n",
    "\n",
    "    @step(pass_context=True)\n",
    "    async def judge_query(self, ctx: Context, ev: StartEvent | JudgeEvent ) -> BadQueryEvent | NaiveRAGEvent | HighTopKEvent | RerankEvent:\n",
    "\n",
    "        # initialize\n",
    "        if not hasattr(ctx.data, \"llm\"):\n",
    "            ctx.data[\"llm\"] = OpenAI(model=\"gpt-4o\",temperature=0.1)\n",
    "            ctx.data[\"index\"] = self.load_or_create_index(\n",
    "                \"data/get-started/\",\n",
    "                \"storage\"\n",
    "            )\n",
    "            # we use a chat engine so it remembers previous interactions\n",
    "            ctx.data[\"judge\"] = SimpleChatEngine.from_defaults()\n",
    "\n",
    "        response = ctx.data[\"judge\"].chat(f\"\"\"\n",
    "            Given a user query, determine if this is likely to yield good results from a RAG system as-is. If it's good, return 'good', if it's bad, return 'bad'.\n",
    "            Good queries use a lot of relevant keywords and are detailed. Bad queries are vague or ambiguous.\n",
    "\n",
    "            Here is the query: {ev.query}\n",
    "            \"\"\")\n",
    "        if response == \"bad\":\n",
    "            # try again\n",
    "            return BadQueryEvent(query=ev.query)\n",
    "        else:\n",
    "            # send query to all 3 strategies\n",
    "            self.send_event(NaiveRAGEvent(query=ev.query))\n",
    "            self.send_event(HighTopKEvent(query=ev.query))\n",
    "            self.send_event(RerankEvent(query=ev.query))\n",
    "\n",
    "    @step(pass_context=True)\n",
    "    async def improve_query(self, ctx: Context, ev: BadQueryEvent) -> JudgeEvent:\n",
    "        response = ctx.data[\"llm\"].complete(f\"\"\"\n",
    "            This is a query to a RAG system: {ev.query}\n",
    "\n",
    "            The query is bad because it is too vague. Please provide a more detailed query that includes specific keywords and removes any ambiguity.\n",
    "        \"\"\")\n",
    "        return JudgeEvent(query=str(response))\n",
    "\n",
    "    @step(pass_context=True)\n",
    "    async def naive_rag(self, ctx: Context, ev: NaiveRAGEvent) -> ResponseEvent:\n",
    "        index = ctx.data[\"index\"]\n",
    "        engine = index.as_query_engine(similarity_top_k=5)\n",
    "        response = engine.query(ev.query)\n",
    "        print(\"Naive response:\", response)\n",
    "        return ResponseEvent(query=ev.query, source=\"Naive\", response=str(response))\n",
    "\n",
    "    @step(pass_context=True)\n",
    "    async def high_top_k(self, ctx: Context, ev: HighTopKEvent) -> ResponseEvent:\n",
    "        index = ctx.data[\"index\"]\n",
    "        engine = index.as_query_engine(similarity_top_k=20)\n",
    "        response = engine.query(ev.query)\n",
    "        print(\"High top k response:\", response)\n",
    "        return ResponseEvent(query=ev.query, source=\"High top k\", response=str(response))\n",
    "\n",
    "    @step(pass_context=True)\n",
    "    async def rerank(self, ctx: Context, ev: RerankEvent) -> ResponseEvent:\n",
    "        index = ctx.data[\"index\"]\n",
    "        reranker = RankGPTRerank(\n",
    "            top_n=5,\n",
    "            llm=ctx.data[\"llm\"]\n",
    "        )\n",
    "        retriever = index.as_retriever(similarity_top_k=20)\n",
    "        engine = RetrieverQueryEngine.from_args(\n",
    "            retriever=retriever,\n",
    "            node_postprocessors=[reranker],\n",
    "        )\n",
    "        response = engine.query(ev.query)\n",
    "        print(\"Reranker response:\", response)\n",
    "        return ResponseEvent(query=ev.query, source=\"Reranker\", response=str(response))\n",
    "\n",
    "    @step(pass_context=True)\n",
    "    async def judge(self, ctx: Context, ev: ResponseEvent) -> StopEvent:\n",
    "        ready = ctx.collect_events(ev, [ResponseEvent]*3)\n",
    "        if ready is None:\n",
    "            return None\n",
    "\n",
    "        response = ctx.data[\"judge\"].chat(f\"\"\"\n",
    "            A user has provided a query and 3 different strategies have been used\n",
    "            to try to answer the query. Your job is to decide which strategy best\n",
    "            answered the query. The query was: {ev.query}\n",
    "\n",
    "            Response 1 ({ready[0].source}): {ready[0].response}\n",
    "            Response 2 ({ready[1].source}): {ready[1].response}\n",
    "            Response 3 ({ready[2].source}): {ready[2].response}\n",
    "\n",
    "            Please provide the number of the best response (1, 2, or 3).\n",
    "            Just provide the number, with no other text or preamble.\n",
    "        \"\"\")\n",
    "\n",
    "        best_response = int(str(response))\n",
    "        print(f\"Best response was number {best_response}, which was from {ready[best_response-1].source}\")\n",
    "        return StopEvent(result=str(ready[best_response-1].response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "complicated_workflow.html\n"
     ]
    }
   ],
   "source": [
    "draw_all_possible_flows(ComplicatedWorkflow,filename=\"complicated_workflow.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running step judge_query\n",
      "Loading existing index...\n",
      "Step judge_query produced no event\n",
      "Running step naive_rag\n",
      "Naive response: To install llama_index on cloud, you would typically need to follow the specific installation instructions provided by the llama_index software documentation or the cloud platform's guidelines for installing third-party software. This may involve using package managers, running specific commands, configuring settings, and ensuring compatibility with the cloud environment.\n",
      "Step naive_rag produced event ResponseEvent\n",
      "Running step rerank\n",
      "Reranker response: To install llama_index on cloud, you would typically need to follow the specific installation instructions provided by the llama_index software documentation or the cloud platform's documentation. This usually involves steps such as setting up the necessary environment, installing any dependencies, configuring the software, and deploying it on the cloud platform according to their guidelines.\n",
      "Step rerank produced event ResponseEvent\n",
      "Running step high_top_k\n",
      "High top k response: To install llama_index on cloud, you would typically need to follow the specific installation instructions provided by the developers or the documentation for llama_index. This may involve steps such as setting up a cloud environment, installing any necessary dependencies, configuring the application, and deploying it to the cloud platform of your choice. It's important to refer to the official documentation or resources related to llama_index for the most accurate and up-to-date installation instructions.\n",
      "Step high_top_k produced event ResponseEvent\n",
      "Running step judge\n",
      "Step judge produced no event\n",
      "Running step judge\n",
      "Step judge produced no event\n",
      "Running step judge\n",
      "Best response was number 3, which was from High top k\n",
      "Step judge produced event StopEvent\n",
      "To install llama_index on cloud, you would typically need to follow the specific installation instructions provided by the developers or the documentation for llama_index. This may involve steps such as setting up a cloud environment, installing any necessary dependencies, configuring the application, and deploying it to the cloud platform of your choice. It's important to refer to the official documentation or resources related to llama_index for the most accurate and up-to-date installation instructions.\n"
     ]
    }
   ],
   "source": [
    "c = ComplicatedWorkflow(timeout=120, verbose=True)\n",
    "result = await c.run(\n",
    "    query=\"How to install llama_index on cloud?\"\n",
    ")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "autogen",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
